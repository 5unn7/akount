# Bank Import Feature - Performance Review Report

**Date**: 2026-01-30
**Reviewer**: Performance Optimization Expert
**Scope**: Bank Import Feature (CSV/PDF Upload, Parsing, Categorization, Duplication Detection)

---

## Executive Summary

### Performance Assessment

- **Risk Level**: üü° HIGH
- **Projected Scale at 10x (10K rows)**: CRITICAL performance degradation expected
- **Projected Scale at 100x (100K rows)**: System failure probable

### Key Findings

**CRITICAL ISSUES** (üî¥): 1 issue
- O(n¬≤) internal duplicate detection will cause timeout at 1K+ rows

**HIGH PRIORITY** (üü°): 4 issues
- N+1 query in account suggestion algorithm
- Missing database indexes on Transaction.date and Transaction.accountId composite
- In-memory parseCache will not scale in multi-server environment
- Large transaction arrays held in memory during entire request lifecycle

**MEDIUM PRIORITY** (üü¢): 3 issues
- Categorization keyword matching could be optimized with trie data structure
- String similarity calculations on every comparison (no caching)
- PDF text extraction loads entire file into memory

**LOW PRIORITY** (‚ö™): 2 issues
- Cache cleanup interval could be more aggressive (5 min ‚Üí 1 min)
- Preview generation includes all row data (could be limited to visible columns)

---

## Performance Metrics

### Current Performance Estimate

| Transaction Count | Expected Time | Database Queries | Memory Usage |
|------------------|---------------|------------------|--------------|
| 100 rows         | 1-2s          | ~10              | ~5MB         |
| 1,000 rows       | **15-30s** ‚ö†Ô∏è | ~12              | ~50MB        |
| 10,000 rows      | **8-15 min** üî¥ | ~15              | ~500MB       |
| Concurrent (10 users √ó 100 rows) | Degradation likely | ~100 | ~50MB      |

### Target Performance (After Optimization)

| Transaction Count | Target Time | Max Queries | Max Memory |
|------------------|-------------|-------------|------------|
| 100 rows         | <1s         | 5           | <5MB       |
| 1,000 rows       | <5s         | 5           | <25MB      |
| 10,000 rows      | <30s        | 5           | <100MB     |
| Concurrent (10 users √ó 100 rows) | No degradation | 50 | <50MB |

---

## Detailed Findings

---

## üî¥ CRITICAL ISSUES

### 1. O(n¬≤) Internal Duplicate Detection

**File**: `apps/api/src/services/duplicationService.ts:203-232`

**Issue**: Nested loop checks every transaction against every other transaction

```typescript
// ‚ùå BAD: O(n¬≤) complexity
export function findInternalDuplicates(
  transactions: ParsedTransaction[]
): Map<string, string[]> {
  const duplicateGroups = new Map<string, string[]>();

  for (let i = 0; i < transactions.length; i++) {      // O(n)
    const groups: string[] = [];

    for (let j = i + 1; j < transactions.length; j++) { // O(n)
      const t1 = transactions[i];
      const t2 = transactions[j];

      if (
        t1.date === t2.date &&
        t1.amount === t2.amount &&
        normalizeDescription(t1.description) ===
          normalizeDescription(t2.description)
      ) {
        groups.push(t2.tempId);
      }
    }
    // ...
  }
  return duplicateGroups;
}
```

**Impact**:
- 100 rows: 4,950 comparisons (acceptable)
- 1,000 rows: 499,500 comparisons (~5-10s)
- 10,000 rows: 49,995,000 comparisons (~10+ minutes) üî¥

**Algorithmic Complexity**: O(n¬≤) time, O(n) space

**Recommendation**: Use hash map for O(n) complexity

```typescript
// ‚úì GOOD: O(n) with hash map
export function findInternalDuplicates(
  transactions: ParsedTransaction[]
): Map<string, string[]> {
  const duplicateGroups = new Map<string, string[]>();
  const seen = new Map<string, string[]>(); // Hash: "date|amount|desc" ‚Üí [tempIds]

  for (const txn of transactions) {
    const key = `${txn.date}|${txn.amount}|${normalizeDescription(txn.description)}`;

    if (!seen.has(key)) {
      seen.set(key, []);
    }
    seen.get(key)!.push(txn.tempId);
  }

  // Find groups with duplicates
  for (const [key, tempIds] of seen.entries()) {
    if (tempIds.length > 1) {
      // First item is the original, rest are duplicates
      duplicateGroups.set(tempIds[0], tempIds.slice(1));
    }
  }

  return duplicateGroups;
}
```

**Expected Improvement**:
- 1,000 rows: 30s ‚Üí <1s (30x faster)
- 10,000 rows: 10 min ‚Üí <5s (120x faster)

---

## üü° HIGH PRIORITY ISSUES

### 2. N+1 Query in Account Suggestion Algorithm

**File**: `apps/api/src/routes/import.ts:270-353`

**Issue**: Account suggestion queries all entities with accounts and transactions, creating nested loops

```typescript
// ‚ùå BAD: Over-fetching data
const entities = await prisma.entity.findMany({
  where: { tenantId },
  include: {
    accounts: {
      where: { isActive: true },
      include: {
        transactions: {                    // Fetches last transaction for EVERY account
          include: { importBatch: true },  // Nested include - heavy query
          take: 1,
          orderBy: { createdAt: 'desc' },
        },
      },
    },
  },
});

// Then loops through all entities and accounts (in-memory joins)
for (const entity of entities) {
  for (const acc of entity.accounts) {
    // Scoring logic...
  }
}
```

**Impact**:
- Tenant with 10 entities √ó 5 accounts = 50 accounts queried
- Each account fetches 1 transaction + importBatch (N+1 style)
- Total: 1 entity query + 50 implicit transaction queries = **51 database roundtrips**

**Database Query Complexity**: O(entities √ó accounts) queries

**Recommendation**: Fetch only necessary data with selective queries

```typescript
// ‚úì GOOD: Targeted query with only needed fields
const accountsWithMetadata = await prisma.account.findMany({
  where: {
    entity: { tenantId },
    isActive: true,
  },
  select: {
    id: true,
    name: true,
    type: true,
    currency: true,
    entityId: true,
    entity: {
      select: {
        id: true,
        name: true,
      },
    },
    transactions: {
      select: {
        importBatch: {
          select: {
            metadata: true,
          },
        },
      },
      take: 1,
      orderBy: { createdAt: 'desc' },
    },
  },
});

// Now only 1 query instead of 51!
```

**Expected Improvement**:
- Query time: 500ms ‚Üí 50ms (10x faster)
- Database load: 51 queries ‚Üí 1 query

---

### 3. Missing Database Indexes

**File**: `packages/db/prisma/schema.prisma:686-722`

**Issue**: Transaction model lacks composite index for duplicate detection query

```prisma
model Transaction {
  // ...
  @@index([accountId, date])             // EXISTS ‚úì
  @@index([accountId, createdAt])        // EXISTS ‚úì
  @@index([categoryId])                  // EXISTS ‚úì
  @@index([sourceType, sourceId])        // EXISTS ‚úì
  // MISSING: composite index for duplicate detection
}
```

**Duplicate Detection Query** (from `duplicationService.ts:55-69`):
```typescript
const existingTransactions = await prisma.transaction.findMany({
  where: {
    accountId,           // ‚úì Uses index
    date: {
      gte: minDate,      // ‚ö†Ô∏è Range scan - partial index usage
      lte: maxDate,
    },
  },
  select: {
    id: true,
    date: true,
    description: true,
    amount: true,
  },
});
```

**Current Index Usage**:
- PostgreSQL will use `@@index([accountId, date])`
- But range queries on `date` prevent full index optimization

**Impact**:
- 1,000 transactions in date range: ~50-100ms (acceptable)
- 10,000 transactions in date range: ~500ms-1s (slow)
- 100,000 transactions in date range: ~5-10s (critical) üî¥

**Recommendation**: Add optimized composite index

```prisma
model Transaction {
  // ... existing fields ...

  @@index([accountId, date])              // Keep for general queries
  @@index([accountId, createdAt])         // Keep
  @@index([categoryId])                   // Keep
  @@index([sourceType, sourceId])         // Keep
  @@index([accountId, date, amount])      // NEW: For duplicate detection (covers amount filter)
  @@index([accountId, description])       // NEW: For description searches (if needed)
}
```

**Expected Improvement**:
- 10,000 row query: 500ms ‚Üí 50ms (10x faster)
- Enables better query planning for amount exact match optimization

---

### 4. In-Memory Parse Cache (Multi-Server Issue)

**File**: `apps/api/src/routes/import.ts:27-50`

**Issue**: In-memory Map will not work in multi-server/load-balanced environment

```typescript
// ‚ùå BAD: In-memory cache (single server only)
const parseCache = new Map<string, {
  accountId: string;
  fileName: string;
  // ... large parsed data ...
  transactions: ParsedTransaction[];  // Could be 10,000+ objects
  createdAt: Date;
}>();

// Cleanup every 5 minutes
setInterval(() => {
  const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000);
  for (const [parseId, data] of parseCache.entries()) {
    if (data.createdAt < oneHourAgo) {
      parseCache.delete(parseId);
    }
  }
}, 5 * 60 * 1000);
```

**Problems**:
1. **Multi-server**: User uploads on Server A, confirms on Server B ‚Üí 404 error
2. **Memory growth**: 10 concurrent users √ó 1,000 rows √ó 5 min = ~50MB held in memory
3. **No persistence**: Server restart = all cached data lost
4. **No memory limits**: Unbounded growth if cleanup fails

**Impact**:
- Horizontal scaling blocked (can't add more servers)
- Memory leak potential if cleanup fails
- Poor user experience with 404 errors on confirm step

**Recommendation**: Replace with Redis for distributed caching

```typescript
// ‚úì GOOD: Redis with TTL and memory limits
import Redis from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST,
  maxRetriesPerRequest: 3,
  maxmemory: '256mb',           // Limit memory
  maxmemoryPolicy: 'allkeys-lru', // Evict oldest when full
});

// Store with 1-hour TTL
async function cacheParseData(parseId: string, data: any) {
  await redis.setex(
    `parse:${parseId}`,
    3600, // 1 hour TTL (auto-cleanup)
    JSON.stringify(data)
  );
}

// Retrieve
async function getCachedParseData(parseId: string) {
  const data = await redis.get(`parse:${parseId}`);
  return data ? JSON.parse(data) : null;
}

// Cleanup is automatic via TTL - no manual interval needed!
```

**Expected Improvement**:
- Enables horizontal scaling (multi-server support)
- Automatic cleanup (no manual interval)
- Bounded memory usage (256MB max)
- Persistence across server restarts

---

### 5. Large Transaction Arrays in Memory

**File**: `apps/api/src/routes/import.ts:178-256`

**Issue**: Entire transaction array held in memory during processing

```typescript
// Parse file
const parseResult = parseCSV(fileBuffer, undefined, dateFormat); // Returns ALL transactions

// Run duplicate detection (holds all in memory)
let duplicateResults: any[] = [];
if (accountId) {
  duplicateResults = await findDuplicates(parseResult.transactions, accountId);
}
const internalDuplicates = findInternalDuplicates(parseResult.transactions);

// Run categorization (holds all in memory)
const categorySuggestions = await categorizeTransactions(
  parseResult.transactions.map(t => ({ description: t.description, amount: t.amount })),
  tenantId
);

// Merge everything (holds all in memory)
const enrichedTransactions = parseResult.transactions.map((t, index) => { ... });
```

**Memory Usage**:
- 100 rows: ~500KB (acceptable)
- 1,000 rows: ~5MB (acceptable)
- 10,000 rows: ~50MB (concerning)
- 100,000 rows: ~500MB (critical) üî¥

**Problem**: No streaming/batching - all operations are on full array

**Recommendation**: Implement batching for large imports

```typescript
// ‚úì GOOD: Process in batches
const BATCH_SIZE = 500; // Process 500 rows at a time

async function processBatch(transactions: ParsedTransaction[], startIdx: number, batchSize: number) {
  const batch = transactions.slice(startIdx, startIdx + batchSize);

  // Process batch
  const duplicates = await findDuplicates(batch, accountId);
  const categories = await categorizeTransactions(batch, tenantId);

  return { duplicates, categories };
}

// Stream processing
for (let i = 0; i < parseResult.transactions.length; i += BATCH_SIZE) {
  const batchResults = await processBatch(parseResult.transactions, i, BATCH_SIZE);
  // Store batch results incrementally
}
```

**Expected Improvement**:
- 10,000 rows: 500MB ‚Üí 25MB (20x memory reduction)
- Enables progress reporting (batch 1/20, 2/20, etc.)
- Prevents timeout on large imports

---

## üü¢ MEDIUM PRIORITY ISSUES

### 6. Categorization Keyword Matching Inefficiency

**File**: `apps/api/src/services/categorizationService.ts:211-297`

**Issue**: Linear search through 100+ keywords for every transaction

```typescript
// ‚ùå BAD: O(keywords √ó transactions) = O(100 √ó n)
for (const transaction of transactions) {
  const normalizedDesc = transaction.description.toLowerCase().trim();
  let bestMatch = null;

  // Check keyword patterns
  for (const [keyword, type, categoryName] of KEYWORD_PATTERNS) { // 100+ keywords
    if (normalizedDesc.includes(keyword)) {  // String search on every keyword
      // ...
    }
  }
}
```

**Complexity**:
- 1 transaction √ó 100 keywords = 100 string searches
- 1,000 transactions √ó 100 keywords = 100,000 string searches (~2-5s)
- 10,000 transactions √ó 100 keywords = 1,000,000 string searches (~20-50s)

**Recommendation**: Use trie data structure for O(1) keyword lookup

```typescript
// ‚úì GOOD: Build keyword trie once at startup
import Trie from 'trie-search';

const keywordTrie = new Trie(['keyword'], {
  ignoreCase: true,
  splitOnRegEx: /\s+/,
});

// Build trie once
for (const [keyword, type, categoryName] of KEYWORD_PATTERNS) {
  keywordTrie.add({ keyword, type, categoryName });
}

// Fast lookup: O(m) where m = description length
export async function categorizeTransactions(
  transactions: Array<{ description: string; amount: number }>,
  tenantId: string
): Promise<CategorySuggestion[]> {
  // ... get categories once ...

  const suggestions: CategorySuggestion[] = [];

  for (const transaction of transactions) {
    // Trie search returns matches in O(m) time
    const matches = keywordTrie.search(transaction.description);

    const bestMatch = matches.length > 0 ? matches[0] : null;
    // ... rest of logic ...
  }

  return suggestions;
}
```

**Expected Improvement**:
- 1,000 transactions: 5s ‚Üí 500ms (10x faster)
- 10,000 transactions: 50s ‚Üí 5s (10x faster)

---

### 7. String Similarity Recalculation

**File**: `apps/api/src/services/duplicationService.ts:145-148`

**Issue**: `compareTwoStrings()` called for every comparison (expensive)

```typescript
// ‚ùå BAD: Recalculates Levenshtein distance every time
for (const existing of existingTransactions) {
  // ...
  const similarity = compareTwoStrings(
    normalizeDescription(transaction.description),
    normalizeDescription(existing.description)
  );
  // Levenshtein algorithm is O(m √ó n) where m,n = string lengths
}
```

**Complexity**:
- 1 imported transaction vs 100 existing = 100 similarity calculations
- 1,000 imported vs 1,000 existing = 1,000,000 calculations (worst case)
- Each calculation is O(m √ó n) string comparison (~50-200 chars)

**Impact**:
- 1,000 imported √ó 500 existing = 500,000 comparisons (~5-10s)

**Recommendation**:
1. Early exit on amount mismatch (already implemented ‚úì)
2. Cache normalized descriptions
3. Consider simpler hash-based similarity for first pass

```typescript
// ‚úì GOOD: Cache normalized descriptions
function findBestMatch(
  transaction: ParsedTransaction,
  existingTransactions: ExistingTransaction[]
): { confidence: number; transactionId?: string; reason?: string } {

  // Pre-normalize once
  const normalizedImportDesc = normalizeDescription(transaction.description);

  // Pre-filter by amount (fast)
  const amountMatches = existingTransactions.filter(
    ex => ex.amount === transaction.amount
  );

  if (amountMatches.length === 0) {
    return { confidence: 0 };
  }

  // Only calculate similarity on amount matches
  for (const existing of amountMatches) {
    const similarity = compareTwoStrings(
      normalizedImportDesc,
      normalizeDescription(existing.description)
    );
    // ... scoring logic ...
  }
}
```

**Expected Improvement**:
- Amount pre-filter reduces comparisons by 90%+ (only compare same amount)
- Caching normalized strings saves repeated normalization

---

### 8. PDF Memory Usage

**File**: `apps/api/src/services/parserService.ts:297-333`

**Issue**: PDF text extraction loads entire file into memory

```typescript
// ‚ùå Potential issue: pdf-parse loads full file into memory
export async function parsePDF(
  fileBuffer: Buffer,  // Entire 10MB file in memory
  dateFormat?: string
): Promise<ParseResult> {
  try {
    const data = await pdf(fileBuffer); // Loads entire PDF into memory
    const text = data.text;             // Full text extracted at once

    // Then parses line by line
    const transactions = parseTransactionsFromPDFText(text, dateFormat);
    // ...
  }
}
```

**Impact**:
- 1MB PDF ‚Üí ~3MB memory (text extraction overhead)
- 10MB PDF ‚Üí ~30MB memory
- Concurrent uploads: 10 users √ó 10MB = 300MB memory spike

**Current Mitigation**: 10MB file size limit (line 83-90) ‚úì

**Recommendation**: This is acceptable for MVP with 10MB limit. Consider streaming for Phase 2.

---

## ‚ö™ LOW PRIORITY ISSUES

### 9. Cache Cleanup Interval

**File**: `apps/api/src/routes/import.ts:43-50`

**Issue**: Cache cleanup runs every 5 minutes, data TTL is 1 hour

```typescript
setInterval(() => {
  const oneHourAgo = new Date(Date.now() - 60 * 60 * 1000);
  for (const [parseId, data] of parseCache.entries()) {
    if (data.createdAt < oneHourAgo) {
      parseCache.delete(parseId);
    }
  }
}, 5 * 60 * 1000); // Cleanup every 5 minutes
```

**Impact**: Data sits in memory for up to 65 minutes (1 hour + 5 min cleanup lag)

**Recommendation**: More aggressive cleanup (1 minute interval) or move to Redis (auto TTL)

---

### 10. Preview Data Overhead

**File**: `apps/api/src/services/parserService.ts:71-74`

**Issue**: Preview includes all columns from first 5 rows

```typescript
const preview = {
  rows: rows.slice(0, 5), // All columns included
};
```

**Impact**: Minimal (only 5 rows), but could be optimized to only include mapped columns

**Recommendation**: Low priority - acceptable for MVP

---

## Database Query Performance Analysis

### Duplicate Detection Query

**Current Implementation** (`duplicationService.ts:55-69`):
```typescript
const existingTransactions = await prisma.transaction.findMany({
  where: {
    accountId,
    date: {
      gte: minDate,
      lte: maxDate,
    },
  },
  select: {
    id: true,
    date: true,
    description: true,
    amount: true,
  },
});
```

**Query Plan Analysis**:
```sql
-- PostgreSQL query equivalent
SELECT id, date, description, amount
FROM "Transaction"
WHERE "accountId" = $1
  AND "date" >= $2
  AND "date" <= $3;

-- Current index: @@index([accountId, date])
-- Query plan: Index Scan on Transaction_accountId_date_idx
-- Estimated rows: 100-10,000 (depends on date range)
```

**Performance**:
- Good: Uses index on `accountId, date`
- Issue: Range scan on date may scan thousands of rows
- Missing: No index on `amount` for exact match optimization

**Recommendation**: Add composite index with amount

```prisma
@@index([accountId, date, amount])
```

This allows PostgreSQL to use "Index Only Scan" when filtering by all three fields.

---

### Categorization Query

**Current Implementation** (`categorizationService.ts:216-222`):
```typescript
const categories = await prisma.category.findMany({
  where: { tenantId },
  select: {
    id: true,
    name: true,
  },
});
```

**Query Plan Analysis**:
```sql
-- PostgreSQL query equivalent
SELECT id, name
FROM "Category"
WHERE "tenantId" = $1;

-- No explicit tenantId index on Category model!
-- Query plan: Sequential Scan (SLOW for large category tables)
```

**Missing Index**: Category table lacks `tenantId` index

**Recommendation**: Add index to Category model

```prisma
model Category {
  id               String             @id @default(cuid())
  tenantId         String             // NEW FIELD: Add tenant isolation
  tenant           Tenant             @relation(fields: [tenantId], references: [id])
  name             String
  // ... rest of fields ...

  @@index([tenantId])        // NEW INDEX
  @@index([tenantId, type])  // NEW COMPOSITE INDEX
  @@index([type])            // Existing
  @@index([isActive])        // Existing
}
```

---

### Account Suggestion Query

**Current Implementation** (`import.ts:271-285`):
```typescript
const entities = await prisma.entity.findMany({
  where: { tenantId },
  include: {
    accounts: {
      where: { isActive: true },
      include: {
        transactions: {
          include: { importBatch: true },
          take: 1,
          orderBy: { createdAt: 'desc' },
        },
      },
    },
  },
});
```

**Query Plan**:
- 1 query for entities
- N queries for accounts (Prisma batches these)
- N queries for transactions (Prisma batches these)
- N queries for importBatches (Prisma batches these)

**Total Queries**: 3-4 database roundtrips (batched), but fetches TOO MUCH data

**Recommendation**: See issue #2 above for optimized query

---

## Scalability Concerns

### 1. Multi-Server Deployment

**Current State**: ‚ùå Blocked by in-memory cache

**Blockers**:
- In-memory `parseCache` Map
- No session affinity/sticky sessions

**Required Changes**:
- Replace parseCache with Redis (issue #4)
- Add session state persistence
- Test with 2+ server instances

---

### 2. Database Connection Pooling

**Current State**: Not reviewed (Prisma default)

**Prisma Default**:
- Connection pool size: ~10 connections per instance
- Timeout: 10 seconds

**Recommendation**: Configure explicit pool limits

```typescript
// packages/db/index.ts
import { PrismaClient } from '@prisma/client';

export const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
  // Connection pool configuration
  __internal: {
    engine: {
      maxQueryExecutionTime: 5000, // 5s timeout
      connectionLimit: 20,          // Max 20 connections per instance
    },
  },
});
```

---

### 3. File Upload Concurrency

**Current State**: No rate limiting on `/import/upload`

**Risk**:
- 10 concurrent users √ó 10MB files = 100MB+ memory spike
- 10 concurrent users √ó 1,000 rows = 10,000 DB queries in parallel

**Recommendation**: Add rate limiting

```typescript
import rateLimit from '@fastify/rate-limit';

await fastify.register(rateLimit, {
  max: 5,              // Max 5 requests
  timeWindow: '1 minute', // Per minute
  keyGenerator: (req) => req.userId, // Per user
});

// Apply to import route
fastify.post('/import/upload', {
  onRequest: [authMiddleware],
  config: {
    rateLimit: {
      max: 3,         // Max 3 uploads per user per minute
      timeWindow: 60000,
    },
  },
  // ... handler ...
});
```

---

## Performance Optimization Roadmap

### Phase 1: Critical Fixes (Week 1)

**Must-fix before production launch**

1. ‚úÖ Fix O(n¬≤) internal duplicate detection (issue #1)
   - Priority: CRITICAL
   - Effort: 2 hours
   - Impact: 120x speedup at 10K rows

2. ‚úÖ Add database indexes (issue #3)
   - Priority: HIGH
   - Effort: 1 hour (migration)
   - Impact: 10x faster duplicate queries

3. ‚úÖ Optimize account suggestion query (issue #2)
   - Priority: HIGH
   - Effort: 3 hours
   - Impact: 10x faster, 50 fewer queries

**Estimated Total**: 6 hours

---

### Phase 2: Scalability (Week 2)

**Required for multi-server deployment**

4. ‚úÖ Replace in-memory cache with Redis (issue #4)
   - Priority: HIGH
   - Effort: 8 hours (Redis setup + testing)
   - Impact: Enables horizontal scaling

5. ‚úÖ Add batch processing for large imports (issue #5)
   - Priority: MEDIUM
   - Effort: 6 hours
   - Impact: 20x memory reduction

6. ‚úÖ Add rate limiting
   - Priority: MEDIUM
   - Effort: 2 hours
   - Impact: Prevents abuse

**Estimated Total**: 16 hours

---

### Phase 3: Performance Tuning (Week 3)

**Nice-to-have optimizations**

7. ‚úÖ Optimize categorization with trie (issue #6)
   - Priority: MEDIUM
   - Effort: 4 hours
   - Impact: 10x faster categorization

8. ‚úÖ Cache string similarity calculations (issue #7)
   - Priority: LOW
   - Effort: 3 hours
   - Impact: 2-3x faster duplicate matching

9. ‚úÖ Add performance monitoring
   - Priority: MEDIUM
   - Effort: 4 hours
   - Impact: Observability

**Estimated Total**: 11 hours

---

### Total Effort: ~33 hours (~1 week of focused work)

---

## Recommended Code Changes

### 1. Optimized Internal Duplicate Detection

**File**: `apps/api/src/services/duplicationService.ts:203-232`

Replace entire function with:

```typescript
/**
 * Find duplicates within the imported batch (before comparing to database)
 *
 * Optimized with hash map for O(n) complexity instead of O(n¬≤)
 */
export function findInternalDuplicates(
  transactions: ParsedTransaction[]
): Map<string, string[]> {
  const duplicateGroups = new Map<string, string[]>();
  const seen = new Map<string, string[]>();

  // Build hash map: "date|amount|normalized_desc" -> [tempIds]
  for (const txn of transactions) {
    const key = `${txn.date}|${txn.amount}|${normalizeDescription(txn.description)}`;

    if (!seen.has(key)) {
      seen.set(key, []);
    }
    seen.get(key)!.push(txn.tempId);
  }

  // Identify duplicate groups (groups with 2+ items)
  for (const [key, tempIds] of seen.entries()) {
    if (tempIds.length > 1) {
      // First occurrence is the "original", rest are duplicates
      duplicateGroups.set(tempIds[0], tempIds.slice(1));
    }
  }

  return duplicateGroups;
}
```

---

### 2. Optimized Account Suggestion Query

**File**: `apps/api/src/routes/import.ts:270-353`

Replace entire section with:

```typescript
// Optimized account suggestion (1 query instead of N)
let suggestedAccounts: any[] = [];
if (!accountId && parseResult.externalAccountData && tenantId) {
  // Fetch all active accounts with last import batch metadata
  const accountsWithMetadata = await prisma.account.findMany({
    where: {
      entity: { tenantId },
      isActive: true,
    },
    select: {
      id: true,
      name: true,
      type: true,
      currency: true,
      entityId: true,
      entity: {
        select: {
          id: true,
          name: true,
        },
      },
      transactions: {
        select: {
          importBatch: {
            select: {
              metadata: true,
            },
          },
        },
        where: {
          importBatch: {
            isNot: null,
          },
        },
        take: 1,
        orderBy: { createdAt: 'desc' },
      },
    },
  });

  // Score each account (in-memory - fast)
  const scoredAccounts = accountsWithMetadata
    .map((acc) => {
      let score = 0;
      const reasons: string[] = [];

      const externalData = parseResult.externalAccountData;

      // Match account type
      if (externalData.accountType && acc.type.toLowerCase() === externalData.accountType) {
        score += 20;
        reasons.push('Account type match');
      }

      // Check import batch metadata
      const importBatch = acc.transactions[0]?.importBatch;
      if (importBatch?.metadata) {
        const metadata = importBatch.metadata as any;
        const storedExternalData = metadata.externalAccountData || {};

        // Match masked account number
        if (
          externalData.externalAccountId &&
          storedExternalData.externalAccountId &&
          externalData.externalAccountId.endsWith(
            storedExternalData.externalAccountId.slice(-4)
          )
        ) {
          score += 50;
          reasons.push(`Account number match (${externalData.externalAccountId})`);
        }

        // Match institution name
        if (externalData.institutionName && storedExternalData.institutionName) {
          const normalizedExternal = externalData.institutionName.toLowerCase();
          const normalizedStored = storedExternalData.institutionName.toLowerCase();

          if (
            normalizedExternal.includes(normalizedStored) ||
            normalizedStored.includes(normalizedExternal)
          ) {
            score += 30;
            reasons.push('Institution match');
          }
        }
      }

      return score > 0
        ? {
            id: acc.id,
            name: acc.name,
            type: acc.type,
            currency: acc.currency,
            entity: acc.entity,
            matchScore: score,
            matchReasons: reasons,
          }
        : null;
    })
    .filter((acc): acc is NonNullable<typeof acc> => acc !== null)
    .sort((a, b) => b.matchScore - a.matchScore)
    .slice(0, 5);

  suggestedAccounts = scoredAccounts;
}
```

---

### 3. Database Migration - Add Indexes

**Create migration file**: `packages/db/prisma/migrations/YYYYMMDD_add_performance_indexes/migration.sql`

```sql
-- Add composite index for duplicate detection performance
CREATE INDEX IF NOT EXISTS "Transaction_accountId_date_amount_idx"
ON "Transaction"("accountId", "date", "amount");

-- Add index on ImportBatch for account queries
CREATE INDEX IF NOT EXISTS "ImportBatch_accountId_status_idx"
ON "ImportBatch"("accountId", "status");

-- Add tenantId to Category if missing (schema change required)
-- This requires updating the Category model in schema.prisma first

-- Add index on Category for tenant isolation
-- CREATE INDEX IF NOT EXISTS "Category_tenantId_idx"
-- ON "Category"("tenantId");

-- Add composite index for category type filtering
-- CREATE INDEX IF NOT EXISTS "Category_tenantId_type_idx"
-- ON "Category"("tenantId", "type");
```

**Update schema.prisma**:

```prisma
model Transaction {
  // ... existing fields ...

  @@index([accountId, date])
  @@index([accountId, createdAt])
  @@index([categoryId])
  @@index([sourceType, sourceId])
  @@index([accountId, date, amount])  // NEW: For duplicate detection
}

model ImportBatch {
  // ... existing fields ...

  @@index([tenantId])
  @@index([entityId])
  @@index([accountId])
  @@index([status])
  @@index([accountId, status])  // NEW: For account import history queries
}

model Category {
  id               String             @id @default(cuid())
  tenantId         String?            // NEW: Add tenant isolation (nullable for migration)
  tenant           Tenant?            @relation(fields: [tenantId], references: [id])
  name             String
  type             CategoryType
  // ... rest of fields ...

  @@index([type])
  @@index([isActive])
  @@index([tenantId])              // NEW: Tenant isolation
  @@index([tenantId, type])        // NEW: Tenant + type filtering
}
```

---

### 4. Redis Cache Implementation

**Create new file**: `apps/api/src/lib/cache.ts`

```typescript
import Redis from 'ioredis';

const redis = new Redis({
  host: process.env.REDIS_HOST || 'localhost',
  port: parseInt(process.env.REDIS_PORT || '6379', 10),
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: 3,
  retryStrategy(times) {
    const delay = Math.min(times * 50, 2000);
    return delay;
  },
});

// Handle Redis connection errors
redis.on('error', (err) => {
  console.error('Redis connection error:', err);
});

export interface ParseCacheData {
  accountId: string;
  fileName: string;
  fileSize: number;
  sourceType: 'CSV' | 'PDF' | 'OFX' | 'XLSX';
  columns?: string[];
  columnMappings?: any;
  preview?: any;
  transactions: any[];
  externalAccountData?: any;
  createdAt: string; // ISO date string
}

/**
 * Store parsed data in Redis with 1-hour TTL
 */
export async function cacheParseData(
  parseId: string,
  data: ParseCacheData
): Promise<void> {
  const cacheKey = `parse:${parseId}`;
  await redis.setex(cacheKey, 3600, JSON.stringify(data)); // 1 hour TTL
}

/**
 * Retrieve parsed data from Redis
 */
export async function getCachedParseData(
  parseId: string
): Promise<ParseCacheData | null> {
  const cacheKey = `parse:${parseId}`;
  const data = await redis.get(cacheKey);
  return data ? JSON.parse(data) : null;
}

/**
 * Delete cached parse data (e.g., after import confirmation)
 */
export async function deleteCachedParseData(parseId: string): Promise<void> {
  const cacheKey = `parse:${parseId}`;
  await redis.del(cacheKey);
}

/**
 * Health check for Redis connection
 */
export async function checkRedisHealth(): Promise<boolean> {
  try {
    await redis.ping();
    return true;
  } catch (error) {
    return false;
  }
}

export { redis };
```

**Update**: `apps/api/src/routes/import.ts`

```typescript
// Replace Map with Redis cache
import { cacheParseData, getCachedParseData, deleteCachedParseData } from '../lib/cache';

// Remove in-memory cache and cleanup interval
// const parseCache = new Map(...); // DELETE
// setInterval(...); // DELETE

// In upload handler (line 356-368), replace:
// parseCache.set(parseId, { ... });
await cacheParseData(parseId, {
  accountId,
  fileName,
  fileSize,
  sourceType,
  columns: parseResult.columns,
  columnMappings: parseResult.suggestedMappings,
  preview: parseResult.preview,
  transactions: enrichedTransactions,
  externalAccountData: parseResult.externalAccountData,
  createdAt: new Date().toISOString(),
});

// In GET /parse/:parseId handler (line 477), replace:
// const cached = parseCache.get(parseId);
const cached = await getCachedParseData(parseId);

// In confirm import handler (add after successful import):
await deleteCachedParseData(parseId); // Clean up after import
```

---

## Testing Recommendations

### Performance Benchmarks

Create test file: `apps/api/src/test/performance/import-benchmark.test.ts`

```typescript
import { parseCSV } from '../../services/parserService';
import { findDuplicates, findInternalDuplicates } from '../../services/duplicationService';
import { categorizeTransactions } from '../../services/categorizationService';

describe('Bank Import Performance Benchmarks', () => {
  const sizes = [100, 1000, 10000];

  for (const size of sizes) {
    describe(`${size} transactions`, () => {
      let transactions: any[];

      beforeAll(() => {
        // Generate test CSV
        transactions = generateTestTransactions(size);
      });

      test(`parseCSV should complete in <${size * 0.01}s`, async () => {
        const start = performance.now();
        const result = parseCSV(Buffer.from(generateCSV(transactions)));
        const duration = performance.now() - start;

        expect(duration).toBeLessThan(size * 10); // 10ms per 1000 rows
        expect(result.transactions.length).toBe(size);
      });

      test(`findInternalDuplicates should complete in <${size * 0.005}s (O(n))`, () => {
        const start = performance.now();
        const duplicates = findInternalDuplicates(transactions);
        const duration = performance.now() - start;

        expect(duration).toBeLessThan(size * 5); // 5ms per 1000 rows (linear)
      });

      test(`categorizeTransactions should complete in <${size * 0.01}s`, async () => {
        const start = performance.now();
        const categories = await categorizeTransactions(transactions, 'test-tenant');
        const duration = performance.now() - start;

        expect(duration).toBeLessThan(size * 10); // 10ms per 1000 rows
      });
    });
  }
});

function generateTestTransactions(count: number) {
  const transactions = [];
  for (let i = 0; i < count; i++) {
    transactions.push({
      tempId: `temp_${i}`,
      date: '2024-01-01',
      description: `Transaction ${i}`,
      amount: Math.floor(Math.random() * 10000),
      isDuplicate: false,
    });
  }
  return transactions;
}

function generateCSV(transactions: any[]): string {
  let csv = 'Date,Description,Amount\n';
  for (const txn of transactions) {
    csv += `${txn.date},${txn.description},${txn.amount / 100}\n`;
  }
  return csv;
}
```

**Expected Results**:

| Test | 100 rows | 1,000 rows | 10,000 rows |
|------|----------|------------|-------------|
| parseCSV | <10ms | <100ms | <1s |
| findInternalDuplicates | <5ms | <50ms | <500ms |
| categorizeTransactions | <10ms | <100ms | <1s |
| **Total** | **<25ms** | **<250ms** | **<2.5s** |

---

### Load Testing

Create artillery config: `apps/api/test/load/import-load-test.yml`

```yaml
config:
  target: 'http://localhost:3001'
  phases:
    - duration: 60
      arrivalRate: 5  # 5 uploads per second
      name: "Sustained load"
  payload:
    path: "test-transactions.csv"
    fields:
      - "csvData"

scenarios:
  - name: "Import CSV"
    flow:
      - post:
          url: "/api/import/upload"
          headers:
            Authorization: "Bearer {{ $processEnvironment.TEST_TOKEN }}"
          formData:
            file: "{{ csvData }}"
            accountId: "{{ $randomString() }}"
            dateFormat: "MM/DD/YYYY"
```

**Run**:
```bash
npm install -g artillery
artillery run apps/api/test/load/import-load-test.yml
```

**Expected Results**:
- p95 latency: <5s (1,000 rows)
- p99 latency: <10s (1,000 rows)
- Error rate: <1%

---

## Approval Status

### Status: üü° OPTIMIZATION RECOMMENDED

### Performance Rating: NEEDS IMPROVEMENT

**Summary**:
- Current implementation will NOT scale to 10,000+ rows
- Critical O(n¬≤) algorithm must be fixed before production
- Database indexes required for acceptable query performance
- Multi-server deployment blocked by in-memory cache

**Recommendation**:
- Complete Phase 1 (critical fixes) before production launch
- Complete Phase 2 (scalability) before multi-server deployment
- Phase 3 can be deferred to post-launch optimization

---

## Next Steps

1. ‚úÖ Prioritize fixing O(n¬≤) internal duplicate detection (2 hours)
2. ‚úÖ Add database indexes via migration (1 hour)
3. ‚úÖ Optimize account suggestion query (3 hours)
4. ‚ö†Ô∏è Set up Redis infrastructure (8 hours)
5. ‚ö†Ô∏è Implement batch processing (6 hours)
6. ‚ö†Ô∏è Add performance monitoring/metrics
7. ‚ö†Ô∏è Run load tests to validate improvements

**Estimated Timeline**: 1-2 weeks for all optimizations

---

## Conclusion

The bank import feature has a solid foundation but requires performance optimizations before handling production-scale workloads. The most critical issue is the O(n¬≤) duplicate detection algorithm, which will cause timeouts at 1,000+ rows.

After implementing the recommended Phase 1 fixes, the system should handle:
- 1,000 rows in <5 seconds ‚úì
- 10,000 rows in <30 seconds ‚úì
- Concurrent uploads without degradation ‚úì

**Total effort to production-ready**: ~33 hours (~1 week)

---

**Report Generated**: 2026-01-30
**Reviewed Files**: 5 files, 1,705 lines of code
**Issues Found**: 10 (1 critical, 4 high, 3 medium, 2 low)
